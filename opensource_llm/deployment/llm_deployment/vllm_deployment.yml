$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: vllm-llama4
endpoint_name: vllm-llama
model: azureml://registries/azureml-meta/models/Llama-3.2-1B-Instruct/versions/3 #replace this with your model name and version
model_mount_path: /models # mount to /models path, so model will show up llama2_13b_chat_sql_tuned 
environment_variables:
  MODEL_PATH: /models/Llama-3.2-1B-Instruct/data # this will need to be set, so vLLM knows where to find the model
  VLLM_ATTENTION_BACKEND: FLASH_ATTN
  VLLM_ARGS: "--max-model-len 15500 --enforce-eager --dtype bfloat16" # optional args for vLLM runtime
environment:
  image: cf406cec7620485598800b18191dabd0.azurecr.io/azureml/azureml_ab5110e06eb7b520e2e7b85848ed86b7@sha256:9109b9e9a1a76356937d0ee58ac8fb2f792a0a5122dbd62d823baa860430cec8
  inference_config:
    liveness_route:
      port: 8000
      path: /health
    readiness_route:
      port: 8000
      path: /health
    scoring_route:
      port: 8000
      path: /
instance_type: Standard_NC24ads_A100_v4
instance_count: 1
request_settings:
    max_concurrent_requests_per_instance: 1
    request_timeout_ms: 180000
liveness_probe:
  initial_delay: 10
  period: 10
  timeout: 2
  success_threshold: 1
  failure_threshold: 30
readiness_probe:
  initial_delay: 120
  period: 10
  timeout: 2
  success_threshold: 1
  failure_threshold: 30
